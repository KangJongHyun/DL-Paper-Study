# DL-Paper-Study

# ${\textsf{\color{purple}Attention is All You Need}}$

**Author**   
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, Illia Polosukhin  

ì§€ë°°ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” Sequence ë³€í™˜ ëª¨ë¸ì€ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ í¬í•¨í•œ RNN í˜¹ì€ CNNì— ê¸°ë°˜í•˜ê³  ìˆë‹¤. í˜„ì¬(2017ë…„) ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì€ **Attention** ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ **ì¸ì½”ë”ì™€ ë””ì½”ë”**ë¥¼ ì—°ê²°í•œë‹¤. ë³¸ Paperì—ì„œëŠ” recuurenceì™€ convolutionì„ ì—†ì• ê³ , Attention ë©”ì»¤ë‹ˆì¦˜ë§Œ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ transformer ì•„í‚¤í…ì³ ëª¨ë¸ì„ ì œì•ˆí•œë‹¤. 
**Model Architecture**  
<img src="https://github.com/user-attachments/assets/e6953b5e-696c-47ee-b4be-af844c1ec5ec" width="300" height="500"/>

ê¸°ì¡´ neural sequence ëª¨ë¸ì€ Encode-Decoder êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ë³´í†µ ì¸ì½”í„°ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ $(x_1, ... x_n)$ì—ì„œ ì—°ì†ì ì¸ í‘œí˜„ ì‹œí€€ìŠ¤ $z=z_1, ... , z_n$ìœ¼ë¡œ ë§¤í•‘í•œë‹¤. ì´í›„ ë””ì½”ë”ëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ $(y_1, ... , y_m)&ì„ ìƒì„±í•œë‹¤. ê° ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ Auto regressive í•˜ë©° Transformerì—ì„œëŠ” ì¸í† ë”ì™€ ë””ì½”ë” ëª¨ë‘ self-attention ë©”ì»¤ë‹ˆì¦˜ê³¼ Point wise Fully connection ì„ ì‚¬ìš©í•˜ì—¬ ì•„í‚¤í…ì³ë¥¼ êµ¬ì„±í•œë‹¤.  

ğŸ¤‘ **ì¸ì½”ë”ì™€ ë””ì½”ë”, Attention ë©”ì»¤ë‹ˆì¦˜ì´ë€?** ğŸ¤‘  
ì°¸ê³  : https://glee1228.tistory.com/3  
Attention Mechanism : 

